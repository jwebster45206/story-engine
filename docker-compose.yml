services:
  roleplay-agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: roleplay-agent
    ports:
      - "8080:8080"
    environment:
      - ROLEPLAY_CONFIG=/app/config.docker.json
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - roleplay-network

  ollama:
      image: ollama/ollama
      container_name: ollama
      ports:
        - "11434:11434"         # Exposes Ollama's HTTP API
      volumes:
        - ollama_data:/root/.ollama
      environment:
        - OLLAMA_MODELS=/root/.ollama/models
      command: >
        sh -c "
        ollama serve &
        sleep 10 &&
        ollama pull tinyllama &&
        wait
        "
      healthcheck:
        test: ["CMD", "sh", "-c", "nc -z localhost 11434 || exit 1"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 120s
      restart: unless-stopped
      networks:
        - roleplay-network

# Ollama data volume to persist models and configurations
volumes:
  ollama_data:
    

# Future services can be added here:
# redis:
#   image: redis:7-alpine
#   container_name: roleplay-redis
#   ports:
#     - "6379:6379"
#   volumes:
#     - redis_data:/data
#   restart: unless-stopped
#   networks:
#     - roleplay-network

# llm-service:
#   # Add your LLM service configuration here
#   # depends_on:
#   #   - roleplay-agent

networks:
  roleplay-network:
    driver: bridge

# volumes:
#   redis_data:
